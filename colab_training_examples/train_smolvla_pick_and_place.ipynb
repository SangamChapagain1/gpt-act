{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤— x ðŸ¦¾: Training SmolVLA with LeRobot for Carrot Pick-and-Place\n",
        "\n",
        "Welcome to the **LeRobot SmolVLA training notebook** for the carrot pick-and-place task!\n",
        "\n",
        "This notebook trains a `SmolVLA` policy using your recorded dataset from the `gpt-act` repo.\n",
        "\n",
        "For official Lerobot SmolVLA notebook visit here:\n",
        "https://colab.research.google.com/github/huggingface/notebooks/blob/main/lerobot/training-smolvla.ipynb\n",
        "\n",
        "## Requirements\n",
        "- A HuggingFace dataset repo ID (e.g., `your-username/so101-pick-and-place-carrot`)\n",
        "- Optional: [wandb](https://wandb.ai/) account for training visualization\n",
        "- Recommended: GPU runtime (NVIDIA A100) for faster training\n",
        "\n",
        "## Expected Training Time\n",
        "Training for 20,000 steps takes **~5 hours on an NVIDIA A100**.\n",
        "Note: SmolVLA requires >20k steps for good performance. 20k is an undertrained example.\n",
        "\n",
        "## Instructions\n",
        "1. **Update `--dataset.repo_id`** to match your HuggingFace dataset.\n",
        "2. **Update `--policy.repo_id`** to where you want the trained model uploaded.\n",
        "3. **Update the output directory** if you want to change where checkpoints are saved.\n",
        "4. Run all cells in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install conda\n",
        "Bootstrap a full Conda environment inside Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Google Drive\n",
        "Persist checkpoints across Colab sessions by mounting Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install LeRobot\n",
        "Clone LeRobot, install FFmpeg, and install the base package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/huggingface/lerobot.git\n",
        "!conda install ffmpeg=7.1.1 -c conda-forge\n",
        "!cd lerobot && pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weights & Biases login\n",
        "Log into W&B for experiment tracking (optional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install SmolVLA dependencies\n",
        "Install transformers and other SmolVLA-specific packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cd lerobot && pip install -e \".[smolvla]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Training SmolVLA\n",
        "\n",
        "**UPDATE THESE VALUES:**\n",
        "- `--dataset.repo_id`: Your HuggingFace dataset ID (e.g., `your-username/so101-pick-and-place-carrot`)\n",
        "- `--policy.repo_id`: Where to upload the trained policy (e.g., `your-username/smolvla_so101_pick_and_place_carrot`)\n",
        "- `--output_dir`: Google Drive path for checkpoints\n",
        "\n",
        "**Training Options:**\n",
        "- `--batch_size=64`: Number of samples per training step (reduce if OOM)\n",
        "- `--steps=20000`: Total training steps (increase to 100k-200k for production)\n",
        "- `--rename_map`: Maps camera names from dataset to model's expected names\n",
        "- `--policy.empty_cameras=1`: Tells model to expect 1 empty camera slot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!lerobot-train \\\n",
        "  --policy.path=lerobot/smolvla_base \\\n",
        "  --dataset.repo_id=sangam-101/so101-pick-and-place-carrot \\\n",
        "  --batch_size=64 \\\n",
        "  --steps=20000 \\\n",
        "  --policy.repo_id=sangam-101/smolvla_so101_pick_and_place_carrot \\\n",
        "  --output_dir=/content/drive/MyDrive/lerobot_runs/smolvla_so101_pick_and_place_carrot \\\n",
        "  --job_name=smolvla_so101_pick_and_place_carrot \\\n",
        "  --policy.device=cuda \\\n",
        "  --wandb.enable=true \\\n",
        "  --rename_map='{\"observation.images.top\": \"observation.images.camera1\",\"observation.images.wrist\": \"observation.images.camera2\"}' \\\n",
        "  --policy.empty_cameras=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Login to HuggingFace Hub\n",
        "After training completes, log in to upload the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!hf auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Model to HuggingFace\n",
        "\n",
        "**UPDATE THESE VALUES:**\n",
        "- First argument: Your policy repo ID\n",
        "- Second argument: Path to the trained checkpoint (update step number if different)\n",
        "\n",
        "By default, this uploads the checkpoint at step 20,000. Update if you trained for more steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!hf upload sangam-101/smolvla_so101_pick_and_place_carrot \\\n",
        "  /content/drive/MyDrive/lerobot_runs/smolvla_so101_pick_and_place_carrot/checkpoints/20000/pretrained_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Complete!\n",
        "\n",
        "Your SmolVLA policy is now trained and uploaded to HuggingFace.\n",
        "\n",
        "### Next Steps:\n",
        "1. Update `scripts/run_inference_smolvla_pick_and_place.py` in `gpt-act` with your policy repo ID\n",
        "2. Run inference on your robot:\n",
        "   ```bash\n",
        "   cd gpt-act\n",
        "   source setup.sh\n",
        "   python scripts/run_inference_smolvla_pick_and_place.py\n",
        "   ```\n",
        "\n",
        "**Note:** SmolVLA models trained for only 20k steps are undertrained and may not perform well. For production use, train for 100k-200k steps with a smaller batch size (e.g., 32) if memory allows."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
